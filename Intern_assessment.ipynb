{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train_data_restaurant.tsv', header=None, sep='\\t')\n",
    "df_test=pd.read_csv('test_data_restaurant.tsv', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"text\", \"class\"]\n",
    "df_test.columns = [\"text\", \"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation and lower casing every letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing on test data\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x.lower())\n",
    "df_test['text'] = df_test['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(df_test[ df_test['class'] == 'positive'].size)\n",
    "print(df_test[ df_test['class'] == 'negative'].size)\n",
    "\n",
    "for idx,row in df_test.iterrows():\n",
    "    row[0] = row[0].replace('rt','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "1160\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing on training data\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df['text'] = df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(df[ df['class'] == 'positive'].size)\n",
    "print(df[ df['class'] == 'negative'].size)\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    row[0] = row[0].replace('rt','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenization on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    4,\n",
       "          10,    3,   37,    5,  643, 1251,  399,   49,   33,  849,    5,\n",
       "         930,  413,  483,  524,  236, 1119,    7, 1251, 1932, 1251,   76,\n",
       "        1933,  274,    9,   13,  850,    7,   55,   64,   38, 1010,   48,\n",
       "          88, 1251,  237,   44,   41,  105,    9,  105,  524,  236, 1119]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization on train data\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         28,  44, 115, 194, 228,  22, 787, 497,  17,   2,  49,  19, 498,\n",
       "          1, 368,   6, 788, 499,  19, 369,  23,  14, 166, 789,  25, 790,\n",
       "         12, 136,  41,  14,  11,   4, 370, 283, 500, 501,  92, 167, 791,\n",
       "        371,  23,   2,  49, 229,  46,  20, 792, 793, 500, 501]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization on test data\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df_test['text'].values)\n",
    "X_df_test = tokenizer.texts_to_sequences(df_test['text'].values)\n",
    "X_df_test = pad_sequences(X_df_test)\n",
    "X_df_test[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 128)         256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_4 (Spatia  (None, None, 128)        0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = None))#X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 77) (1780, 2)\n",
      "(185, 76) (185, 2)\n"
     ]
    }
   ],
   "source": [
    "#Y = df['class'].values\n",
    "Y = pd.get_dummies(df['class']).values\n",
    "Y_df_test = pd.get_dummies(df_test['class']).values\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "print(X_df_test.shape,Y_df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14/14 [==============================] - 10s 466ms/step - loss: 0.6599 - accuracy: 0.6612\n",
      "Epoch 2/15\n",
      "14/14 [==============================] - 6s 454ms/step - loss: 0.5941 - accuracy: 0.6742\n",
      "Epoch 3/15\n",
      "14/14 [==============================] - 7s 510ms/step - loss: 0.4601 - accuracy: 0.7663\n",
      "Epoch 4/15\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 0.3014 - accuracy: 0.8792\n",
      "Epoch 5/15\n",
      "14/14 [==============================] - 8s 603ms/step - loss: 0.2283 - accuracy: 0.9191\n",
      "Epoch 6/15\n",
      "14/14 [==============================] - 11s 768ms/step - loss: 0.1557 - accuracy: 0.9416\n",
      "Epoch 7/15\n",
      "14/14 [==============================] - 8s 597ms/step - loss: 0.1239 - accuracy: 0.9584\n",
      "Epoch 8/15\n",
      "14/14 [==============================] - 12s 840ms/step - loss: 0.0819 - accuracy: 0.9725\n",
      "Epoch 9/15\n",
      "14/14 [==============================] - 10s 695ms/step - loss: 0.0639 - accuracy: 0.9809\n",
      "Epoch 10/15\n",
      "14/14 [==============================] - 9s 660ms/step - loss: 0.0525 - accuracy: 0.9848\n",
      "Epoch 11/15\n",
      "14/14 [==============================] - 10s 684ms/step - loss: 0.0405 - accuracy: 0.9888\n",
      "Epoch 12/15\n",
      "14/14 [==============================] - 11s 822ms/step - loss: 0.0344 - accuracy: 0.9893\n",
      "Epoch 13/15\n",
      "14/14 [==============================] - 11s 764ms/step - loss: 0.0411 - accuracy: 0.9871\n",
      "Epoch 14/15\n",
      "14/14 [==============================] - 10s 699ms/step - loss: 0.0298 - accuracy: 0.9927\n",
      "Epoch 15/15\n",
      "14/14 [==============================] - 9s 671ms/step - loss: 0.0376 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128cbe1a670>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X, Y, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_df_test,batch_size = batch_size)\n",
    "classes_x=np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[ 17  48]\n",
      " [ 19 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.26      0.34        65\n",
      "           1       0.68      0.84      0.75       120\n",
      "\n",
      "    accuracy                           0.64       185\n",
      "   macro avg       0.58      0.55      0.54       185\n",
      "weighted avg       0.61      0.64      0.61       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'true': Y_df_test.tolist(), 'pred':classes_x})\n",
    "results['true'] = results['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(results.true, results.pred))\n",
    "print(classification_report(results.true, results.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary and notes for furthur improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What kind of preprocessing is done and the reason behind it\n",
    "    There are 3 main preprocessing techniques that I have used\n",
    "        - Punctuation removal: Punctuation marks such as commas, colons, and double quotes also have no effect on finding the context of the sentence for the model. Therefore, punctuation marks will be removed.\n",
    "        - Lower case sentences: Uppercase and lowercase letters only help in structuring the sentences for humans, but for the model uppercase and lowercase words can be seen differently by the model but they have the same meaning so they can be grouped as the same word.\n",
    "        Tokenization: The tokenization process divides words into tokens, which are then used as input in the normalization and cleaning processes. It can also be used to turn text into numerical form, which machine learning models can understand.\n",
    "        \n",
    "- Methods used to solve the problem and the reason behind it: \n",
    "    There are various multiple reaserch that indicates that LSTM models out-performs any other deep learning model when it comes to text classification. Although recently there have been research which started applying hybrid model such as mixing CNN+LSTM for better performance.\n",
    "\n",
    "- Performance metrics used to test the model and the reason behind it: \n",
    "    I created a confusion matrix table which has all of the important metrics which can be used to evaluate every aspect of the category of the class that have predicted by the model. In this model, the model predicts 'positive' class much better as indicated by the precision and recall metrics. \n",
    "    \n",
    "- Model performance analysis: \n",
    "    The model will most of the time predict the class 'positive' and there is a high chance that most of the negative sentiment will be classified as positive as shown by the confusion matrix and the metrics avialable within the confsuion matrix. The reason the model is acting this way is because the dataset is imblanced, in order to fix it, we need to apply downsampling to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assessment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000128CBC4BB20> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"assessment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
